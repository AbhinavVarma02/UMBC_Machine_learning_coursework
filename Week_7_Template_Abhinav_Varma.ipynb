{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dd-BTPFD4MBH"
      },
      "source": [
        "# Preamble\n",
        "This problem set is an extension of Problem Set 6.  You will need the MNIST 784 dataset from OpenML, with dimensionality reduced to about 75\\% of original variance.\n",
        "\n",
        "As with last week, the first 60,000 observations are available to use as training data, and the remaining 10,000 images as test data.  In training the models, you do not need to use all 60,000 observations.  (It is suggested to partition the training data into a training dataset and holdout dataset rather than use cross-validation.  Training on as few as 5000 observations is sufficient to reduce training time.)\n",
        "\n",
        "For purposes of this problem set, recode the target variable for both the test and training sets to classify whether a digit is less than 5 (i.e., $y \\in \\left\\{0, 1, 2, 3, 4\\right\\}$).  That is, the target variable should take the value 0 where the corresponding observation depicts a 0, 1, 2, 3, or 4; and the value 1 where the corresponding observation depicts a 5, 6, 7, 8, or 9.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IJwiByZP_8aN"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import fetch_openml\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from sklearn.model_selection import cross_val_predict\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import VotingClassifier\n",
        "from sklearn.metrics import f1_score\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Load the MNIST dataset\n",
        "X, y = fetch_openml('mnist_784', version=1, return_X_y=True, as_frame=False)\n",
        "#Perform dimensionality reduction\n",
        "pca = PCA(0.75)\n",
        "X_reduced = pca.fit_transform(X)\n",
        "#Split the dataset into training and test sets\n",
        "X_train, X_test = X_reduced[:60000], X_reduced[60000:]\n",
        "y_train, y_test = y[:60000], y[60000:]\n",
        "#Partition training data into a training dataset and a holdout dataset\n",
        "N_train = 5000\n",
        "X_train_dataset, X_holdout_dataset = X_train[:N_train], X_train[N_train:]\n",
        "y_train_dataset, y_holdout_dataset = y_train[:N_train], y_train[N_train:]\n",
        "#Recode the target variable for training and test sets\n",
        "recode = lambda y: np.where(np.isin(y, ['0','1','2','3','4']), 0, 1)\n",
        "y_train_rcd = recode(y_train)\n",
        "y_test_rcd = recode(y_test)"
      ],
      "metadata": {
        "id": "TFGbNe_gCEfo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5tPtoB1Y3v77"
      },
      "source": [
        "# Problem 1 -- Classifiers\n",
        "\n",
        "Train 3 classifiers on the dataset, each using a different algorithm.  Each classifier must have an $F_1$ score of at least 0.9.  At least one classifier must use gradient boosting (AdaBoost, Gradient Boost, or xgboost).  Show the $F_1$ score and classification report for each model."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import f1_score, classification_report\n",
        "\n",
        "# Classifier 1: Decision Tree Classifier (Similar to Random Forest with a single tree)\n",
        "dt_classifier = DecisionTreeClassifier(random_state=42)\n",
        "dt_classifier.fit(X_train, y_train_rcd)\n",
        "\n",
        "dt_y_pred = dt_classifier.predict(X_test)\n",
        "\n",
        "dt_f1_score = f1_score(y_test_rcd, dt_y_pred)\n",
        "print(\"Decision Tree Classifier:\")\n",
        "print(\"F1 Score:\", dt_f1_score)\n",
        "print(classification_report(y_test_rcd, dt_y_pred))\n"
      ],
      "metadata": {
        "id": "P1v2xp6Y2plc",
        "outputId": "e01ba75e-df63-414c-8d6b-00150c410ab2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Decision Tree Classifier:\n",
            "F1 Score: 0.8973407544836116\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.90      0.91      0.90      5139\n",
            "           1       0.90      0.90      0.90      4861\n",
            "\n",
            "    accuracy                           0.90     10000\n",
            "   macro avg       0.90      0.90      0.90     10000\n",
            "weighted avg       0.90      0.90      0.90     10000\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from xgboost import XGBClassifier\n",
        "xgb_clf = XGBClassifier(n_estimators=150, learning_rate=0.1, use_label_encoder=False, eval_metric='logloss')\n",
        "xgb_clf.fit(X_train_dataset, y_train_rcd[:N_train])\n",
        "y_pred_xgb = xgb_clf.predict(X_test)\n",
        "f1_xgb = f1_score(y_test_rcd, y_pred_xgb)\n",
        "print(\"\\nXGBoost Classifier:\")\n",
        "print(classification_report(y_test_rcd, y_pred_xgb))\n",
        "print(f\"F1 Score: {f1_xgb:.4f}\")"
      ],
      "metadata": {
        "id": "0TbhL86L5hxt",
        "outputId": "8d4c3811-58e4-46e9-8b1d-f05c738e41c7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [19:58:49] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "XGBoost Classifier:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.95      0.93      0.94      5139\n",
            "           1       0.92      0.95      0.94      4861\n",
            "\n",
            "    accuracy                           0.94     10000\n",
            "   macro avg       0.94      0.94      0.94     10000\n",
            "weighted avg       0.94      0.94      0.94     10000\n",
            "\n",
            "F1 Score: 0.9362\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "# Train Random Forest classifier with optimized parameters\n",
        "rf_clf = RandomForestClassifier(n_estimators=200, max_depth=15, random_state=42)\n",
        "rf_clf.fit(X_train_dataset, y_train_rcd[:N_train])\n",
        "y_pred_rf = rf_clf.predict(X_test)\n",
        "f1_rf = f1_score(y_test_rcd, y_pred_rf)\n",
        "print(\"Random Forest Classifier:\")\n",
        "print(classification_report(y_test_rcd, y_pred_rf))\n",
        "print(f\"F1 Score: {f1_rf:.4f}\")"
      ],
      "metadata": {
        "id": "x6B139XM5yBB",
        "outputId": "22269200-1071-411d-e7ff-875184f6fff7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random Forest Classifier:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.96      0.92      0.94      5139\n",
            "           1       0.92      0.95      0.94      4861\n",
            "\n",
            "    accuracy                           0.94     10000\n",
            "   macro avg       0.94      0.94      0.94     10000\n",
            "weighted avg       0.94      0.94      0.94     10000\n",
            "\n",
            "F1 Score: 0.9356\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LG5qDkzu4uRS"
      },
      "source": [
        "# Problem 2 -- Voting ensemble model\n",
        "\n",
        "(20 pts) Build a voting ensemble model that combines the three classifiers from the previous problem, in addition to the SVM model developed last week.  What is the $F_1$ score of the ensemble model?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QIReirN0418z",
        "outputId": "c93484b4-2b95-4e8c-922b-9567adacae75",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [20:02:32] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ensemble Model F1 Score: 0.9684471024953598\n"
          ]
        }
      ],
      "source": [
        "from sklearn.svm import SVC\n",
        "svm_classifier = SVC(probability=True, random_state=42)\n",
        "svm_classifier.fit(X_train, y_train_rcd)\n",
        "#creating a voting classifier\n",
        "voting_classifier = VotingClassifier(estimators=[\n",
        "    ('dt', dt_classifier),\n",
        "    ('xgb', xgb_clf),\n",
        "    ('rf', rf_clf),\n",
        "    ('svm', svm_classifier)\n",
        "], voting='hard')\n",
        "\n",
        "voting_classifier.fit(X_train, y_train_rcd)\n",
        "voting_predictions = voting_classifier.predict(X_test)\n",
        "ensemble_f1_score = f1_score(y_test_rcd, voting_predictions)\n",
        "print(\"Ensemble Model F1 Score:\", ensemble_f1_score)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B2Pnq0i042SD"
      },
      "source": [
        "## Problem 3 -- Stacking ensemble model\n",
        "Stacking uses a final classifier (often a logistic regression) that outputs an aggregate of the predictors. Repeat the previous problem using a StackingClassifier rather than voting to compute the final prediction.  What is the $F_1$ score of the stacking classifier?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cQQ6Y1zG6K3m",
        "outputId": "3f63464b-4bdc-48de-b710-c05c08d22399",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stacking Classifier F1 Score: 0.9825862957238537\n"
          ]
        }
      ],
      "source": [
        "from sklearn.ensemble import StackingClassifier, RandomForestClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "# Initialize individual base classifiers\n",
        "dt_classifier = DecisionTreeClassifier(random_state=42)\n",
        "xgb_clf = XGBClassifier(random_state=42)\n",
        "rf_clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "svm_classifier = SVC(probability=True, random_state=42)\n",
        "\n",
        "# Creating a StackingClassifier with the base classifiers and final estimator (logistic regression)\n",
        "stacking_classifier = StackingClassifier(\n",
        "    estimators=[\n",
        "        ('dt', dt_classifier),\n",
        "        ('xgb', xgb_clf),\n",
        "        ('rf', rf_clf),\n",
        "        ('svm', svm_classifier)\n",
        "    ],\n",
        "    final_estimator=LogisticRegression(),\n",
        "    cv=5\n",
        ")\n",
        "\n",
        "# Train the stacking classifier\n",
        "stacking_classifier.fit(X_train, y_train_rcd)\n",
        "\n",
        "# Make predictions\n",
        "stacking_predictions = stacking_classifier.predict(X_test)\n",
        "\n",
        "# Compute F1 Score\n",
        "stacking_f1_score = f1_score(y_test_rcd, stacking_predictions)\n",
        "\n",
        "print(\"Stacking Classifier F1 Score:\", stacking_f1_score)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}